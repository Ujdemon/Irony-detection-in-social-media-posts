{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Train Sample after cleaning :: 1081\n",
      "Total Test Sample after cleaning :: 3817\n"
     ]
    }
   ],
   "source": [
    "# Import our dependencies\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "from keras import backend as K\n",
    "import keras.layers as layers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report, mean_squared_error, roc_auc_score\n",
    "from keras.engine import Layer\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing import *\n",
    "from tweet_utils import *\n",
    "\n",
    "\n",
    "from  sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Initialize session\n",
    "config = tf.ConfigProto(gpu_options= tf.GPUOptions(allow_growth=True), allow_soft_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "K.set_session(sess)\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "\n",
    "class ElmoEmbeddingLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.dimensions = 1024\n",
    "        self.trainable=False\n",
    "        super(ElmoEmbeddingLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.elmo = hub.Module('https://tfhub.dev/google/elmo/2', trainable=self.trainable,\n",
    "                               name=\"{}_module\".format(self.name))\n",
    "\n",
    "        self.trainable_weights += K.tf.trainable_variables(scope=\"^{}_module/.*\".format(self.name))\n",
    "        super(ElmoEmbeddingLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        result = self.elmo(K.squeeze(K.cast(x, tf.string), axis=1),\n",
    "                      as_dict=True,\n",
    "                      signature='default',\n",
    "                      )['default']\n",
    "        return result\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return K.not_equal(inputs, '--PAD--')\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dimensions)\n",
    "\n",
    "\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length_post):\n",
    "    \n",
    "    in_id_post_enc = layers.Input(shape=(max_seq_length_post,), name=\"input_ids_post\")\n",
    "    \n",
    "    in_id_post = layers.Input(shape=(1,), dtype=\"string\")\n",
    "\n",
    "    post_embed = layers.Embedding(len(tknizer.word_index), 100, input_length=max_seq_length_post)(in_id_post_enc)\n",
    "    \n",
    "    post_lstm = layers.LSTM(300, dropout=0.2, recurrent_dropout=0.5)(post_embed)\n",
    "    \n",
    "    elmo_output_post = ElmoEmbeddingLayer()(in_id_post)\n",
    "    \n",
    "    cat_output_post = layers.concatenate([elmo_output_post, post_lstm], axis=-1)\n",
    "    \n",
    "    batched_output_post = layers.BatchNormalization()(cat_output_post)\n",
    "    \n",
    "    dense_post = layers.Dense(256, activation='relu')(batched_output_post)\n",
    "    \n",
    "\n",
    "    cat_output = layers.Dropout(0.3)(dense_post)\n",
    "\n",
    "    dense = layers.Dense(100, activation='relu')(cat_output)\n",
    "    pred = layers.Dense(1, activation='sigmoid')(dense)\n",
    "    \n",
    "    model = Model(inputs=[in_id_post, in_id_post_enc], outputs=pred)\n",
    "    print('Model Loaded')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"semevaltask3.csv\", sep=',')\n",
    "df_train = pd.read_csv(\"election.csv\",sep = ',')\n",
    "data_test = df_test.dropna(axis = 0, how ='any')\n",
    "data_train = df_train.dropna(axis = 0, how ='any')\n",
    "print('Total Train Sample after cleaning ::', len(data_train))\n",
    "print('Total Test Sample after cleaning ::', len(data_test))\n",
    "tweet_post_test = data_test['Tweet text'].tolist()\n",
    "tweet_post_train = data_train['Tweets'].tolist()\n",
    "\n",
    "label_test = data_test['Label'].tolist()\n",
    "label_train = data_train['Task A'].tolist()\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "\n",
    "def create_tokenizer(p):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(p)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def preprocessed(tokenizer, docs, m_length):\n",
    "    X = tokenizer.texts_to_sequences(docs)\n",
    "    X = pad_sequences(X, padding='post', truncating='post', value = 0, maxlen=m_length)\n",
    "    return X\n",
    "\n",
    "\n",
    "def tweet_cl(df_list):\n",
    "    return [tweet_processor(t).lower() for t in df_list]\n",
    "\n",
    "tweet_post_test = tweet_cl(tweet_post_test)\n",
    "tweet_post_train = tweet_cl(tweet_post_train)\n",
    "\n",
    "tknizer = create_tokenizer(tweet_post_test)\n",
    "tknizer = create_tokenizer(tweet_post_train)\n",
    "\n",
    "post_max_seq_length_test = max([len(post.split()) for post in tweet_post_test])\n",
    "post_max_seq_length_train = max([len(post.split()) for post in tweet_post_train])\n",
    "\n",
    "post_encoded_test = preprocessed(tknizer, tweet_post_test, post_max_seq_length_test)\n",
    "post_encoded_train = preprocessed(tknizer, tweet_post_train, post_max_seq_length_train)\n",
    "\n",
    "\n",
    "text_post_test = np.array(tweet_post_test, dtype=object)[:, np.newaxis]\n",
    "text_post_train = np.array(tweet_post_train, dtype=object)[:, np.newaxis]\n",
    "\n",
    "task_a_label_dict_test = {el:idx for idx, el in enumerate(data_test['Label'].unique())}\n",
    "task_a_label_dict_train = {el:idx for idx, el in enumerate(data_train['Task A'].unique())}\n",
    "\n",
    "label_conv_test = [task_a_label_dict_test[l] for l in label_test]\n",
    "label_conv_train = [task_a_label_dict_train[l] for l in label_train]\n",
    "\n",
    "\n",
    "train_post_text, test_post_text = text_post_train,text_post_test\n",
    "train_labela, test_labela = label_conv_train,label_conv_test\n",
    "\n",
    "train_post_enc, test_post_enc = post_encoded_train,post_encoded_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3817, 67)\n"
     ]
    }
   ],
   "source": [
    "print(test_post_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1081, 370)\n"
     ]
    }
   ],
   "source": [
    "print(train_post_enc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids_post (InputLayer)     (None, 370)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 370, 100)     638200      input_ids_post[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "elmo_embedding_layer_6 (ElmoEmb (None, 1024)         4           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   (None, 300)          481200      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1324)         0           elmo_embedding_layer_6[0][0]     \n",
      "                                                                 lstm_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 1324)         5296        concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 256)          339200      batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 256)          0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 100)          25700       dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 1)            101         dense_17[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 1,489,701\n",
      "Trainable params: 1,487,053\n",
      "Non-trainable params: 2,648\n",
      "__________________________________________________________________________________________________\n",
      "Train on 864 samples, validate on 217 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,256,57,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_6/elmo_embedding_layer_6_module_apply_default/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss_5/mul/_2093]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,256,57,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_6/elmo_embedding_layer_6_module_apply_default/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-232f30663d81>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m           verbose=2)\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[32,256,57,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_6/elmo_embedding_layer_6_module_apply_default/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[loss_5/mul/_2093]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[32,256,57,46] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node elmo_embedding_layer_6/elmo_embedding_layer_6_module_apply_default/bilm/CNN/Conv2D_4}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."
     ]
    }
   ],
   "source": [
    "model = build_model(post_max_seq_length_train)\n",
    "\n",
    "\n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "cb = EarlyStopping(monitor='val_loss', patience=3)\n",
    "mc = ModelCheckpoint('ElmoSimpleModel.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "model.fit([train_post_text, train_post_enc],\n",
    "          train_labela,\n",
    "          validation_split=0.2,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=[cb, mc],\n",
    "          verbose=2)\n",
    "\n",
    "\n",
    "post_save_preds = model.predict([test_post_text, test_post_enc]) # predictions after we clear and reload model\n",
    "prediction = [1 if ps >0.5 else 0 for ps in post_save_preds]\n",
    "\n",
    "# print(prediction)\n",
    "# In[ ]:\n",
    "\n",
    "mse = mean_squared_error(test_labela, prediction)\n",
    "print('Mean Squared Error = ' + str(mse))\n",
    "\n",
    "roc_auc = roc_auc_score(test_labela, prediction)\n",
    "print('ROC_AUC Report = ' +str(roc_auc))\n",
    "\n",
    "rep = classification_report(test_labela, prediction, digits=4)\n",
    "print('Classification Report \\n' +str(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"semeval as train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([test_post_text, test_post_enc],\n",
    "          test_labela,\n",
    "#           validation_split=0.2,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          callbacks=[cb, mc],\n",
    "          verbose=2)\n",
    "\n",
    "\n",
    "post_save_preds = model.predict([train_post_text, train_post_enc]) # predictions after we clear and reload model\n",
    "prediction = [1 if ps >0.5 else 0 for ps in post_save_preds]\n",
    "\n",
    "# print(prediction)\n",
    "# In[ ]:\n",
    "\n",
    "mse = mean_squared_error(train_labela, prediction)\n",
    "print('Mean Squared Error = ' + str(mse))\n",
    "\n",
    "roc_auc = roc_auc_score(train_labela, prediction)\n",
    "print('ROC_AUC Report = ' +str(roc_auc))\n",
    "\n",
    "rep = classification_report(train_labela, prediction, digits=4)\n",
    "print('Classification Report \\n' +str(rep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30522, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:138: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96340a7b66ea47d5b113aa1485e91351",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Converting examples to features', max=1081.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc04329ff84efa9aef2d86921bf86d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Converting examples to features', max=3817.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "370\n",
      "210\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <__main__.BertLayer object at 0x0000019E46B88088>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids_post (InputLayer)     [(None, 370)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks_post (InputLayer)   [(None, 370)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids_post (InputLayer)   [(None, 370)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 370, 100)     3052200     input_ids_post[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids_post[0][0]             \n",
      "                                                                 input_masks_post[0][0]           \n",
      "                                                                 segment_ids_post[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 300)          481200      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1068)         0           bert_layer_1[0][0]               \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 1068)         4272        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          273664      batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 100)          25700       dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            101         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 113,942,027\n",
      "Trainable params: 29,134,289\n",
      "Non-trainable params: 84,807,738\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[node checkpoint_initializer_99 (defined at C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:407) ]]\n\nOriginal stack trace for 'checkpoint_initializer_99':\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-2b9d3a87ba9e>\", line 1, in <module>\n    model = build_model(1)\n  File \"<ipython-input-2-f740e6bbc68d>\", line 69, in build_model\n    elmo_output_post = ElmoEmbeddingLayer()(in_id_post)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"<ipython-input-2-f740e6bbc68d>\", line 38, in build\n    name=\"{}_module\".format(self.name))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\", line 170, in __init__\n    tags=self._tags)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 340, in _create_impl\n    name=name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 399, in __init__\n    self._init_state(name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 407, in _init_state\n    self._variable_map)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 291, in init_from_checkpoint\n    init_from_checkpoint_fn)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1684, in merge_call\n    return self._merge_call(merge_fn, args, kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1691, in _merge_call\n    return merge_fn(self._strategy, *args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 286, in <lambda>\n    ckpt_dir_or_file, assignment_map)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 334, in _init_from_checkpoint\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 458, in _set_variable_or_list_initializer\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 412, in _set_checkpoint_initializer\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1355\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1356\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1357\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1341\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1429\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node checkpoint_initializer_99}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-3bf4d65233b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training start'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-3bf4d65233b7>\u001b[0m in \u001b[0;36minitialize_vars\u001b[1;34m(sess)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minitialize_vars\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 950\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    951\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1171\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1173\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1174\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1350\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1368\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1370\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1372\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[node checkpoint_initializer_99 (defined at C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py:407) ]]\n\nOriginal stack trace for 'checkpoint_initializer_99':\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 664, in launch_instance\n    app.start()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 563, in start\n    self.io_loop.start()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 148, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\base_events.py\", line 534, in run_forever\n    self._run_once()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1771, in _run_once\n    handle._run()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\asyncio\\events.py\", line 88, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 787, in inner\n    self.run()\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 541, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2848, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2874, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3242, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-12-2b9d3a87ba9e>\", line 1, in <module>\n    model = build_model(1)\n  File \"<ipython-input-2-f740e6bbc68d>\", line 69, in build_model\n    elmo_output_post = ElmoEmbeddingLayer()(in_id_post)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 431, in __call__\n    self.build(unpack_singleton(input_shapes))\n  File \"<ipython-input-2-f740e6bbc68d>\", line 38, in build\n    name=\"{}_module\".format(self.name))\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\", line 170, in __init__\n    tags=self._tags)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 340, in _create_impl\n    name=name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 399, in __init__\n    self._init_state(name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\", line 407, in _init_state\n    self._variable_map)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 291, in init_from_checkpoint\n    init_from_checkpoint_fn)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1684, in merge_call\n    return self._merge_call(merge_fn, args, kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py\", line 1691, in _merge_call\n    return merge_fn(self._strategy, *args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 286, in <lambda>\n    ckpt_dir_or_file, assignment_map)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 334, in _init_from_checkpoint\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 458, in _set_variable_or_list_initializer\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\checkpoint_utils.py\", line 412, in _set_checkpoint_initializer\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_io_ops.py\", line 1696, in restore_v2\n    name=name)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3616, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\wwwuj\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2005, in __init__\n    self._traceback = tf_stack.extract_stack()\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from bert.tokenization import FullTokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report, mean_squared_error, roc_auc_score\n",
    "\n",
    "import nltk\n",
    "from tweet_utils import *\n",
    "from preprocessing import *\n",
    "\n",
    "nltk_tokeniser = nltk.tokenize.TweetTokenizer()\n",
    "\n",
    "# Initialize session\n",
    "config = tf.ConfigProto(gpu_options= tf.GPUOptions(allow_growth=True), allow_soft_placement=True)\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Params for bert model and tokenization\n",
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"semevaltask3.csv\", sep=',')\n",
    "df_train = pd.read_csv(\"election.csv\",sep = ',')\n",
    "data_test = df_test.dropna(axis = 0, how ='any')\n",
    "data_train = df_train.dropna(axis = 0, how ='any')\n",
    "\n",
    "text_post_train = data_train['Tweets'].tolist()\n",
    "label_train = data_train['Task A'].tolist()\n",
    "text_post_test = data_test['Tweet text'].tolist()\n",
    "label_test = data_test['Label'].tolist()\n",
    "\n",
    "def tweet_cl(df_list):\n",
    "    return [tweet_processor(t).lower() for t in df_list]\n",
    "\n",
    "\n",
    "text_post_train = tweet_cl(text_post_train)\n",
    "text_post_test = tweet_cl(text_post_test)\n",
    "\n",
    "post_max_seq_length_train = max([len(post.split()) for post in text_post_train])\n",
    "post_max_seq_length_test = max([len(post.split()) for post in text_post_test])\n",
    "\n",
    "text_post_train = np.array(text_post_train, dtype=object)[:, np.newaxis]\n",
    "text_post_test = np.array(text_post_test, dtype=object)[:, np.newaxis]\n",
    "\n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "  When running eval/predict on the TPU, we need to pad the number of examples\n",
    "  to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "  size. The alternative is to drop the last batch, which is bad because it means\n",
    "  the entire output data won't be generated.\n",
    "  We use this class instead of `None` because treating `None` as padding\n",
    "  battches could cause silent errors.\n",
    "  \"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    bert_module =  hub.Module(bert_path)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    vocab_file, do_lower_case = sess.run(\n",
    "        [\n",
    "            tokenization_info[\"vocab_file\"],\n",
    "            tokenization_info[\"do_lower_case\"],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )\n",
    "\n",
    "def convert_text_to_examples(texts, labels):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for text, label in zip(texts, labels):\n",
    "        InputExamples.append(\n",
    "            InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "\n",
    "# In[39]:\n",
    "\n",
    "\n",
    "# Instantiate tokenizer\n",
    "tokenizer = create_tokenizer_from_hub_module()\n",
    "\n",
    "def wordembedding(tokenizer):\n",
    "    # load the whole embedding into memory\n",
    "    embeddings_index = dict()\n",
    "    for line in open('glove.6B.100d.txt',encoding=\"utf-8\"):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "    # create a weight matrix for words in training docs\n",
    "    embedding_matrix = np.zeros((len(tokenizer.vocab), 100))\n",
    "    for word, i in tokenizer.vocab.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "\n",
    "glove_word_embedding = wordembedding(tokenizer)\n",
    "print(glove_word_embedding.shape)\n",
    "\n",
    "# from  sklearn.model_selection import train_test_split\n",
    "\n",
    "train_post_text, test_post_text = text_post_train,text_post_test\n",
    "\n",
    "train_label, test_label = label_train,label_test\n",
    "\n",
    "train_post_examples = convert_text_to_examples(train_post_text, train_label)\n",
    "\n",
    "test_post_examples = convert_text_to_examples(test_post_text, test_label)\n",
    "\n",
    "# Convert to features\n",
    "(train_input_ids_post, train_input_masks_post, train_segment_ids_post, train_labels_post) = convert_examples_to_features(tokenizer, train_post_examples, max_seq_length=post_max_seq_length_train)\n",
    "\n",
    "(test_input_ids_post, test_input_masks_post, test_segment_ids_post, test_labels_post) = convert_examples_to_features(tokenizer, test_post_examples, max_seq_length=post_max_seq_length_test)\n",
    "\n",
    "print(post_max_seq_length_train)\n",
    "print(post_max_seq_length_test)\n",
    "print('')\n",
    "\n",
    "\n",
    "\n",
    "class BertLayer(tf.keras.layers.Layer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_fine_tune_layers=10,\n",
    "        pooling=\"first\",\n",
    "        bert_path=\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = False\n",
    "        self.output_size = 768\n",
    "        self.pooling = pooling\n",
    "        self.bert_path = bert_path\n",
    "        if self.pooling not in [\"first\", \"mean\"]:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            self.bert_path, trainable=self.trainable, name=f\"{self.name}_module\"\n",
    "        )\n",
    "\n",
    "        # Remove unused layers\n",
    "        trainable_vars = self.bert.variables\n",
    "        if self.pooling == \"first\":\n",
    "            trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "            trainable_layers = [\"pooler/dense\"]\n",
    "\n",
    "        elif self.pooling == \"mean\":\n",
    "            trainable_vars = [\n",
    "                var\n",
    "                for var in trainable_vars\n",
    "                if not \"/cls/\" in var.name and not \"/pooler/\" in var.name\n",
    "            ]\n",
    "            trainable_layers = []\n",
    "        else:\n",
    "            raise NameError(\n",
    "                f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\"\n",
    "            )\n",
    "\n",
    "        # Select how many layers to fine tune\n",
    "        for i in range(self.n_fine_tune_layers):\n",
    "            trainable_layers.append(f\"encoder/layer_{str(11 - i)}\")\n",
    "\n",
    "        # Update trainable vars to contain only the specified layers\n",
    "        trainable_vars = [\n",
    "            var\n",
    "            for var in trainable_vars\n",
    "            if any([l in var.name for l in trainable_layers])\n",
    "        ]\n",
    "\n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "\n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        if self.pooling == \"first\":\n",
    "            pooled = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"pooled_output\"\n",
    "            ]\n",
    "        elif self.pooling == \"mean\":\n",
    "            result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "                \"sequence_output\"\n",
    "            ]\n",
    "\n",
    "            mul_mask = lambda x, m: x * tf.expand_dims(m, axis=-1)\n",
    "            masked_reduce_mean = lambda x, m: tf.reduce_sum(mul_mask(x, m), axis=1) / (\n",
    "                    tf.reduce_sum(m, axis=1, keepdims=True) + 1e-10)\n",
    "            input_mask = tf.cast(input_mask, tf.float32)\n",
    "            pooled = masked_reduce_mean(result, input_mask)\n",
    "        else:\n",
    "            raise NameError(f\"Undefined pooling type (must be either first or mean, but is {self.pooling}\")\n",
    "\n",
    "        return pooled\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)\n",
    "\n",
    "\n",
    "# In[44]:\n",
    "\n",
    "\n",
    "# Build model\n",
    "def build_model(max_seq_length_post):\n",
    "\n",
    "    in_id_post = tf.keras.layers.Input(shape=(max_seq_length_post,), name=\"input_ids_post\")\n",
    "    in_mask_post = tf.keras.layers.Input(shape=(max_seq_length_post,), name=\"input_masks_post\")\n",
    "    in_segment_post = tf.keras.layers.Input(shape=(max_seq_length_post,), name=\"segment_ids_post\")\n",
    "    bert_inputs_post = [in_id_post, in_mask_post, in_segment_post]\n",
    "\n",
    "\n",
    "#     in_id_des = tf.keras.layers.Input(shape=(max_seq_length_des,), name=\"input_ids_des\")\n",
    "#     in_mask_des = tf.keras.layers.Input(shape=(max_seq_length_des,), name=\"input_masks_des\")\n",
    "#     in_segment_des = tf.keras.layers.Input(shape=(max_seq_length_des,), name=\"segment_ids_des\")\n",
    "#     bert_inputs_des = [in_id_des, in_mask_des, in_segment_des]\n",
    "\n",
    "    post_embed = tf.keras.layers.Embedding(len(tokenizer.vocab), 100, input_length=max_seq_length_post, weights=[glove_word_embedding], trainable=False)(in_id_post)\n",
    "#     des_embed = tf.keras.layers.Embedding(len(tokenizer.vocab), 100, input_length=max_seq_length_des, weights=[glove_word_embedding], trainable=False)(in_id_des)\n",
    "\n",
    "    post_lstm = tf.keras.layers.LSTM(300, dropout=0.2, recurrent_dropout=0.5)(post_embed)\n",
    "#     des_lstm = tf.keras.layers.LSTM(300, dropout=0.2, recurrent_dropout=0.5)(des_embed)\n",
    "\n",
    "\n",
    "    bert_output_post = BertLayer(n_fine_tune_layers=4, pooling=\"mean\")(bert_inputs_post)\n",
    "#     bert_output_des = BertLayer(n_fine_tune_layers=4, pooling=\"mean\")(bert_inputs_des)\n",
    "\n",
    "    cat_output_post = tf.keras.layers.concatenate([bert_output_post, post_lstm], axis=-1)\n",
    "#     cat_output_des = tf.keras.layers.concatenate([bert_output_des, des_lstm], axis=-1)\n",
    "\n",
    "    batched_output_post = tf.keras.layers.BatchNormalization()(cat_output_post)\n",
    "#     batched_output_des = tf.keras.layers.BatchNormalization()(cat_output_des)\n",
    "\n",
    "    dense_post = tf.keras.layers.Dense(256, activation='relu')(batched_output_post)\n",
    "#     dense_des = tf.keras.layers.Dense(256, activation='relu')(batched_output_des)\n",
    "\n",
    "#     cat_output = tf.keras.layers.concatenate([dense_post, dense_des], axis=-1)\n",
    "#     cat_output = tf.keras.layers.Dropout(0.3)(cat_output)\n",
    "\n",
    "    dense = tf.keras.layers.Dense(100, activation='relu')(dense_post)\n",
    "    pred = tf.keras.layers.Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[bert_inputs_post], outputs=pred)\n",
    "    print('Model Loaded')\n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n",
    "\n",
    "\n",
    "# In[45]:\n",
    "\n",
    "\n",
    "model = build_model(post_max_seq_length_train)\n",
    "\n",
    "\n",
    "# In[46]:\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "model.compile(loss='mse', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# In[47]:\n",
    "\n",
    "\n",
    "train_inputs = [train_input_ids_post, train_input_masks_post, train_segment_ids_post]\n",
    "test_inputs = [test_input_ids_post, test_input_masks_post, test_segment_ids_post]\n",
    "\n",
    "\n",
    "# In[48]:\n",
    "\n",
    "cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "mc = tf.keras.callbacks.ModelCheckpoint('BertSimpleModel.h5', monitor='val_loss', verbose=0, save_best_only=True, mode='auto')\n",
    "\n",
    "\n",
    "initialize_vars(sess)\n",
    "\n",
    "print('Training start')\n",
    "model.fit(\n",
    "    train_inputs,\n",
    "    train_labels_post,\n",
    "#     validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[cb, mc],\n",
    "    verbose=2)\n",
    "\n",
    "post_save_preds = model.predict(test_inputs) # predictions after we clear and reload model\n",
    "\n",
    "prediction = [1 if ps >0.5 else 0 for ps in post_save_preds]\n",
    "# print(prediction)\n",
    "# In[ ]:\n",
    "\n",
    "mse = mean_squared_error(test_labels_post, prediction)\n",
    "print('Mean Squared Error = ' + str(mse))\n",
    "\n",
    "roc_auc = roc_auc_score(test_labels_post, prediction)\n",
    "print('ROC_AUC Report = ' +str(roc_auc))\n",
    "\n",
    "rep = classification_report(test_labels_post, prediction, digits=4)\n",
    "print('Classification Report \\n' +str(rep))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"now semeval as train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    test_inputs,\n",
    "    test_labels_post,\n",
    "#     validation_split=0.2,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    callbacks=[cb, mc],\n",
    "    verbose=2)\n",
    "\n",
    "post_save_preds = model.predict(train_inputs) # predictions after we clear and reload model\n",
    "\n",
    "prediction = [1 if ps >0.5 else 0 for ps in post_save_preds]\n",
    "# print(prediction)\n",
    "# In[ ]:\n",
    "\n",
    "mse = mean_squared_error(train_labels_post, prediction)\n",
    "print('Mean Squared Error = ' + str(mse))\n",
    "\n",
    "roc_auc = roc_auc_score(train_labels_post, prediction)\n",
    "print('ROC_AUC Report = ' +str(roc_auc))\n",
    "\n",
    "rep = classification_report(train_labels_post, prediction, digits=4)\n",
    "print('Classification Report \\n' +str(rep))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondaaf27a7a0acc3437ba8317d9194673e70",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"election.csv\", encoding='utf-8')\n",
    "train_data = pd.read_csv(\"SemEval2018-T3-train-taskA.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test_data[\"Tweets\"]\n",
    "train_tweets = train_data[\"Tweet text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_classes = test_data['Task A']\n",
    "train_classes = train_data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = train_tweets.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'url')\n",
    "processed_train = processed_train.str.replace(r'\\d+(\\.\\d+)?', 'number')\n",
    "processed_test = test_tweets.str.replace(r'^http\\://[a-zA-Z0-9\\-\\.]+\\.[a-zA-Z]{2,3}(/\\S*)?$',\n",
    "                                  'url')\n",
    "processed_test = processed_test.str.replace(r'\\d+(\\.\\d+)?', 'number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = processed_train.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed_train = processed_train.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed_train = processed_train.str.replace(r'^\\s+|\\s+?$', '')\n",
    "\n",
    "processed_test = processed_test.str.replace(r'[^\\w\\d\\s]', ' ')\n",
    "\n",
    "# Replace whitespace between terms with a single space\n",
    "processed_test = processed_test.str.replace(r'\\s+', ' ')\n",
    "\n",
    "# Remove leading and trailing whitespace\n",
    "processed_test = processed_test.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train = processed_train.str.lower()\n",
    "processed_test = processed_test.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from text messages\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed_train = processed_train.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))\n",
    "\n",
    "processed_test = processed_test.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed_train = processed_train.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))\n",
    "\n",
    "processed_test = processed_test.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words_train = []\n",
    "all_words_test = []\n",
    "for message in processed_train:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words_train.append(w)\n",
    "        \n",
    "for message in processed_test:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words_test.append(w)\n",
    "        \n",
    "all_words_train = nltk.FreqDist(all_words_train)\n",
    "all_words_test = nltk.FreqDist(all_words_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_features_train = list(all_words_train.keys())\n",
    "word_features_test = list(all_words_test.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features_train(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features_train:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "def find_features_test(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features_test:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the messages\n",
    "messages_train = list(zip(processed_train,train_classes))\n",
    "messages_test = list(zip(processed_test,test_classes))\n",
    "\n",
    "# define a seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages_train)\n",
    "np.random.shuffle(messages_test)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets_train = [(find_features_train(text), label) for (text, label) in messages_train]\n",
    "featuresets_test = [(find_features_test(text), label) for (text, label) in messages_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1081\n3817\n"
    }
   ],
   "source": [
    "print(len(featuresets_test))\n",
    "print(len(featuresets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Define models to train\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\",\"XGBoost\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    XGBClassifier()\n",
    "]\n",
    "\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(featuresets_train)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "         \"Naive Bayes\", \"SVM Linear\",\"XGBoost\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    LogisticRegression(),\n",
    "    SGDClassifier(max_iter = 100),\n",
    "    MultinomialNB(),\n",
    "    SVC(kernel = 'linear'),\n",
    "    XGBClassifier()\n",
    "]\n",
    "models = list(zip(names, classifiers))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(featuresets_train)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, featuresets_test)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_features, labels = zip(*featuresets_test)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['not irony', 'irony']],\n",
    "    columns = [['predicted', 'predicted'], ['not irony', 'irony']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}